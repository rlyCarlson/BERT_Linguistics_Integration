# -*- coding: utf-8 -*-
"""presup_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hsKqBw1ge9jTdvNy1Sz0NVmildYFUGXA
"""

from google.colab import drive
drive.mount('/content/drive')

import gc
gc.collect()
!pip install datasets
!pip install peft
!pip install trl
!pip install torch
!pip install bitsandbytes
!pip install accelerate
!pip install --upgrade accelerate
!pip install datasets rouge-score nltk
!pip install sacrebleu
!pip install evaluate

import os
import json
from datasets import Dataset
from datasets import load_dataset, load_metric

train_dataset_path = "/content/drive/MyDrive/CS224n_finalproj/train_presuppositions.jsonl"
validation_dataset_path = "/content/drive/MyDrive/CS224n_finalproj/val_presuppositions.jsonl"
test_dataset_path = "/content/drive/MyDrive/CS224n_finalproj/test_presuppositions.jsonl"

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:1024"

with open(train_dataset_path, 'r') as file:
    train_data = [json.loads(line) for line in file]

with open(validation_dataset_path, 'r') as file:
    validation_data = [json.loads(line) for line in file]

with open(test_dataset_path, 'r') as file:
    test_data = [json.loads(line) for line in file]

train_dataset = Dataset.from_list(train_data)
train_dataset = train_dataset.rename_column("sentence1", "input")
train_dataset = train_dataset.rename_column("sentence2", "output")

val_dataset = Dataset.from_list(validation_data)
val_dataset = val_dataset.rename_column("sentence1", "input")
val_dataset = val_dataset.rename_column("sentence2", "output")

test_dataset = Dataset.from_list(test_data)
test_dataset = test_dataset.rename_column("sentence1", "input")
test_dataset = test_dataset.rename_column("sentence2", "output")


dataset = {
    'train': train_dataset,
    'validation': val_dataset,
    'test': test_dataset
}

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer
from datasets import load_dataset
from evaluate import load

model_name = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def preprocess_function(examples):
    inputs = examples["input"]
    targets = examples["output"]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return {k: v for k, v in model_inputs.items()}

tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)

tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)

tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)

tokenized_dataset = {
    'train': tokenized_train_dataset,
    'validation': tokenized_val_dataset,
    'test': tokenized_test_dataset
}

bleu = load("bleu")
rouge = load("rouge")

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    if isinstance(labels, tuple):
        labels = labels[0]

    preds = torch.argmax(torch.tensor(preds), dim=-1)

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)
    rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    print("BLEU score:", bleu_score)
    print("ROUGE scores:", rouge_scores)
    return {"bleu": bleu_score["bleu"], "rouge": rouge_scores}

training_args = TrainingArguments(
    output_dir="output",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=20,
    weight_decay=0.01,
    report_to="tensorboard",
    logging_dir="./logs",
    gradient_accumulation_steps=4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    compute_metrics=compute_metrics,
)

print("STARTING TO TRAIN")
trainer.train()

eval_results = trainer.evaluate()
print(eval_results)
print(f"BLEU score: {eval_results['eval_bleu']:.4f}")
print(f"ROUGE scores: {eval_results['eval_rouge']}")

model.save_pretrained("/content/drive/MyDrive/CS224n_finalproj/t5-base-finetuned-presuppositions-small")